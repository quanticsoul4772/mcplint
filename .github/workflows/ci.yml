name: CI

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

env:
  CARGO_TERM_COLOR: always

jobs:
  build:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        rust: [stable]

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          components: clippy, rustfmt

      - name: Cache cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            target/
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

      - name: Install Ollama (Linux)
        if: runner.os == 'Linux'
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama serve &
          # Wait for server to be ready
          for i in {1..30}; do
            if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
              echo "Ollama server is ready"
              break
            fi
            echo "Waiting for Ollama server... ($i/30)"
            sleep 2
          done
          ollama pull llama3.2
          # Warm up the model by loading it into memory - MUST complete before tests
          echo "Warming up llama3.2 model (this loads it into memory)..."
          curl -s --max-time 300 http://localhost:11434/api/generate -d '{"model": "llama3.2", "prompt": "Say hello", "stream": false}'
          echo ""
          echo "Model warm-up complete - model is now loaded"

      - name: Install Ollama (macOS)
        if: runner.os == 'macOS'
        run: |
          brew install ollama
          ollama serve &
          # Wait for server to be ready
          for i in {1..30}; do
            if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
              echo "Ollama server is ready"
              break
            fi
            echo "Waiting for Ollama server... ($i/30)"
            sleep 2
          done
          ollama pull llama3.2
          # Warm up the model by loading it into memory - MUST complete before tests
          echo "Warming up llama3.2 model (this loads it into memory)..."
          curl -s --max-time 300 http://localhost:11434/api/generate -d '{"model": "llama3.2", "prompt": "Say hello", "stream": false}'
          echo ""
          echo "Model warm-up complete - model is now loaded"

      - name: Install Ollama (Windows)
        if: runner.os == 'Windows'
        run: |
          # Download Ollama CLI directly (smaller than full zip)
          $ollamaUrl = "https://github.com/ollama/ollama/releases/download/v0.13.2/ollama-windows-amd64.zip"
          Write-Host "Downloading Ollama from $ollamaUrl"
          Invoke-WebRequest -Uri $ollamaUrl -OutFile "ollama.zip" -TimeoutSec 600

          # Extract
          Expand-Archive -Path "ollama.zip" -DestinationPath "." -Force

          # Verify ollama.exe exists
          if (Test-Path ".\ollama.exe") {
            Write-Host "ollama.exe found"
          } else {
            Write-Host "ERROR: ollama.exe not found after extraction"
            Get-ChildItem -Recurse | Select-Object FullName
            exit 1
          }

          # Start Ollama server in background
          Start-Process -FilePath ".\ollama.exe" -ArgumentList "serve" -WindowStyle Hidden

          # Wait for server to be ready
          $maxRetries = 30
          $retry = 0
          while ($retry -lt $maxRetries) {
            Start-Sleep -Seconds 2
            try {
              $response = Invoke-WebRequest -Uri "http://localhost:11434/api/tags" -TimeoutSec 5 -ErrorAction SilentlyContinue
              if ($response.StatusCode -eq 200) {
                Write-Host "Ollama server is ready"
                break
              }
            } catch {
              Write-Host "Waiting for Ollama server... ($retry/$maxRetries)"
            }
            $retry++
          }

          if ($retry -eq $maxRetries) {
            Write-Host "ERROR: Ollama server failed to start"
            exit 1
          }

          # Pull the model
          .\ollama.exe pull llama3.2

          # Warm up the model by loading it into memory - MUST complete before tests
          Write-Host "Warming up llama3.2 model (this loads it into memory)..."
          $body = '{"model": "llama3.2", "prompt": "Say hello", "stream": false}'
          $response = Invoke-WebRequest -Uri "http://localhost:11434/api/generate" -Method POST -Body $body -ContentType "application/json" -TimeoutSec 300
          Write-Host "Model warm-up complete - model is now loaded"
          Write-Host "Response: $($response.Content)"
        shell: pwsh

      - name: Check formatting
        run: cargo fmt -- --check

      - name: Clippy
        run: cargo clippy -- -D warnings

      - name: Build
        run: cargo build --verbose

      - name: Run tests
        run: cargo test --verbose
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

  release:
    needs: build
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/')

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Build release
        run: cargo build --release

      - name: Create Release
        uses: softprops/action-gh-release@v1
        with:
          files: target/release/mcplint
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
